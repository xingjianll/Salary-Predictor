{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:11.225642Z",
     "start_time": "2024-04-18T10:36:09.249303Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import read_csv_data, clean_location\n",
    "\n",
    "data = read_csv_data(\"../data/processed_job_postings_large.csv\", \n",
    "                     [\"industry\", \"work_type\", \"location\", \"formatted_experience_level\",\n",
    "                      \"name\", \"cleaned_title\", \"cleaned_description\"],\n",
    "                     \"standardized_annual_salary\")\n",
    "data = clean_location(data, 2)\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:20000]\n",
    "val_data = data[20000:30000]\n",
    "test_data = data[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72692612c9cf88a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:14.703238Z",
     "start_time": "2024-04-18T10:36:13.991539Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import build_column_vocabulary\n",
    "\n",
    "vocab_industry = build_column_vocabulary(train_data, 0)\n",
    "vocab_type = build_column_vocabulary(train_data, 1)\n",
    "vocab_state = build_column_vocabulary(train_data, 2)\n",
    "vocab_level = build_column_vocabulary(train_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4fd8830c588f794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:16.080469Z",
     "start_time": "2024-04-18T10:36:15.334263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import convert_to_one_hot\n",
    "import torch\n",
    "\n",
    "train_cat_features = convert_to_one_hot(train_data, \n",
    "                                  [(0, vocab_industry),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "val_cat_features = convert_to_one_hot(val_data, \n",
    "                                  [(0, vocab_industry),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "train_cat_features = torch.stack(train_cat_features)\n",
    "val_cat_features = torch.stack(val_cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d1c3502cb523762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:17.354505Z",
     "start_time": "2024-04-18T10:36:17.143078Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "a = tokenizer('hello this is a test',\n",
    "         truncation=True,\n",
    "         padding='max_length',\n",
    "         max_length=512,\n",
    "         return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f09d5233be3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:22.222134Z",
     "start_time": "2024-04-18T10:36:18.117859Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gpt1 import GPT1Dataset\n",
    "\n",
    "descriptions = [item[0][6] for item in train_data]\n",
    "input_ids, attention_mask = [], []\n",
    "for description in descriptions:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    # input_ids.append(encoding['input_ids'].squeeze())\n",
    "    # attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    input_ids.append(encoding['input_ids'][0])\n",
    "    attention_mask.append(encoding['attention_mask'][0])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "input_ids = torch.stack(input_ids)\n",
    "attention_mask = torch.stack(attention_mask)\n",
    "\n",
    "labels = [float(target) for _, target in train_data]\n",
    "\n",
    "train_dataset = GPT1Dataset(input_ids, attention_mask, train_cat_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc429ee792143ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:25.734327Z",
     "start_time": "2024-04-18T10:36:23.231319Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "descriptions = [item[0][6] for item in val_data]\n",
    "input_ids, attention_mask = [], []\n",
    "for description in descriptions:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    # input_ids.append(encoding['input_ids'].squeeze())\n",
    "    # attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    input_ids.append(encoding['input_ids'][0])\n",
    "    attention_mask.append(encoding['attention_mask'][0])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "input_ids = torch.stack(input_ids)\n",
    "attention_mask = torch.stack(attention_mask)\n",
    "\n",
    "labels = [float(target) for _, target in val_data]\n",
    "\n",
    "val_dataset = GPT1Dataset(input_ids, attention_mask, val_cat_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "206cac4e80e854b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:27.521643Z",
     "start_time": "2024-04-18T10:36:27.505099Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6025166c04034571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:34.993781Z",
     "start_time": "2024-04-18T10:36:33.099022Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d6185ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "def accuracy(model, dataset: Dataset) -> float:\n",
    "    \"\"\"\n",
    "    copied from csc413 lab 1\n",
    "    Compute the accuracy of `model` over the `dataset`.\n",
    "    We will take the **most probable class**\n",
    "    as the class predicted by the model.\n",
    "\n",
    "    Parameters:\n",
    "        `model` - A torch.nn model. We will only be passing `nn.Linear` models.\n",
    "                  However, to make your code more generally useful, do not access\n",
    "                  `model.weight` and `model.bias` parameters directly. These\n",
    "                  class attributes may not exist for other kinds of models.\n",
    "        `dataset` - A list of 2-tuples of the form (x, t), where `x` is a PyTorch\n",
    "                  tensor of shape [1, 28, 28] representing an MNIST image,\n",
    "                  and `t` is the corresponding target label\n",
    "\n",
    "    Returns: a floating-point value between 0 and 1.\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    distance = 0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    for i in range(500):\n",
    "        data = dataset[i]\n",
    "\n",
    "        input_ids = data['input_ids'].unsqueeze(0).to(device)\n",
    "        attention_mask = data['attention_mask'].unsqueeze(0).to(device)\n",
    "        categorical_features = data['categorical_features'].unsqueeze(0).to(device)\n",
    "        label = data['labels']\n",
    "\n",
    "        output = model(input_ids, attention_mask, categorical_features)\n",
    "        output = output.item()\n",
    "\n",
    "        distance += float(abs(label-output))\n",
    "        total += 1\n",
    "\n",
    "    return distance / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e4fbe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "class GPT1Dataset(Dataset):\n",
    "    def __init__(self, input_ids: list[Tensor],\n",
    "                 attention_mask: list[Tensor],\n",
    "                 categorical_features: list[Tensor],\n",
    "                 labels: list[float]):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.categorical_features = categorical_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'categorical_features': self.categorical_features[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6208a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "from utils import plot_results\n",
    "\n",
    "def _collate_batch(batch):\n",
    "    \"\"\"Custom collate function for handling batches of data where all input tensors are of the same length.\"\"\"\n",
    "\n",
    "    # Separate and stack the data directly since all tensors are already of the same length\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    categorical_features = torch.stack([item['categorical_features'] for item in batch]).float()\n",
    "    labels = torch.tensor([item['labels'] for item in batch], dtype=torch.float)\n",
    "\n",
    "    return input_ids, attention_mask, categorical_features, labels\n",
    "\n",
    "def train_model(model,\n",
    "                train_data: GPT1Dataset,\n",
    "                val_data: GPT1Dataset,\n",
    "                learning_rate=0.01,\n",
    "                batch_size=100,\n",
    "                num_epochs=10,\n",
    "                plot_every=50,\n",
    "                plot=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=_collate_batch)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    iters, losses, train_mae, val_mae = [], [], [], []\n",
    "    iter_count = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_ids, attention_mask, categorical_features, label in train_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            categorical_features = categorical_features.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, categorical_features)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion(outputs, label.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (iter_count + 1) % plot_every == 0:\n",
    "                iters.append(iter_count)\n",
    "                losses.append(float(loss))\n",
    "                train_mae.append(accuracy(model, train_data))\n",
    "                val_mae.append(accuracy(model, val_data))\n",
    "                print(\n",
    "                    f\"Iter {iter_count + 1}: Loss: {losses[-1]} Train mae {train_mae[-1]}, Validation mae {val_mae[-1]}\")\n",
    "            iter_count += 1\n",
    "\n",
    "    if plot:\n",
    "        plot_results(iters, losses, train_mae, val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6db1dcc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:37:12.184647Z",
     "start_time": "2024-04-18T10:36:44.799945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Loss: 5656090624.0 Train mae 78833.72181980466, Validation mae 78816.57021628904\n",
      "Iter 100: Loss: 6460871680.0 Train mae 39212.312450937505, Validation mae 39333.97844406249\n",
      "Iter 150: Loss: 1621057280.0 Train mae 40017.46119718749, Validation mae 40411.43692374999\n",
      "Iter 200: Loss: 3103582208.0 Train mae 39988.10675968749, Validation mae 40274.61412687499\n",
      "Iter 250: Loss: 2298023680.0 Train mae 40303.50279874999, Validation mae 39539.14173624999\n",
      "Iter 300: Loss: 942694208.0 Train mae 40214.05067374999, Validation mae 39556.61379093749\n",
      "Iter 350: Loss: 5207917568.0 Train mae 39433.59916593749, Validation mae 39812.808603437494\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgpt1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT1\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT1(\u001b[38;5;28mlen\u001b[39m(vocab_type) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_industry) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_state) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_level))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 48\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data, val_data, learning_rate, batch_size, num_epochs, plot_every, plot)\u001b[0m\n\u001b[0;32m     46\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, label\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 48\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (iter_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m plot_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gpt1 import GPT1\n",
    "\n",
    "model = GPT1(len(vocab_type) + len(vocab_industry) + len(vocab_state) + len(vocab_level))\n",
    "\n",
    "train_model(model, train_data=train_dataset, val_data=train_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49555d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
