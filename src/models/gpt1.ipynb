{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:11.225642Z",
     "start_time": "2024-04-18T10:36:09.249303Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import read_csv_data, clean_location\n",
    "\n",
    "data = read_csv_data(\"../data/processed_job_postings_large.csv\", \n",
    "                     [\"industry\", \"work_type\", \"location\", \"formatted_experience_level\",\n",
    "                      \"name\", \"cleaned_title\", \"cleaned_description\"],\n",
    "                     \"standardized_annual_salary\")\n",
    "data = clean_location(data, 2)\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:100]\n",
    "val_data = data[100:200]\n",
    "test_data = data[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "72692612c9cf88a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:14.703238Z",
     "start_time": "2024-04-18T10:36:13.991539Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import build_column_vocabulary\n",
    "\n",
    "vocab_industry = build_column_vocabulary(train_data, 0)\n",
    "vocab_type = build_column_vocabulary(train_data, 1)\n",
    "vocab_state = build_column_vocabulary(train_data, 2)\n",
    "vocab_level = build_column_vocabulary(train_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4fd8830c588f794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:16.080469Z",
     "start_time": "2024-04-18T10:36:15.334263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import convert_to_one_hot\n",
    "import torch\n",
    "\n",
    "train_cat_features = convert_to_one_hot(train_data, \n",
    "                                  [(0, vocab_industry),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "val_cat_features = convert_to_one_hot(val_data, \n",
    "                                  [(0, vocab_industry),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "test_cat_features = convert_to_one_hot(test_data, \n",
    "                                  [(0, vocab_industry),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "train_cat_features = torch.stack(train_cat_features).to(device)\n",
    "val_cat_features = torch.stack(val_cat_features).to(device)\n",
    "test_cat_features = torch.stack(test_cat_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d1c3502cb523762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:17.354505Z",
     "start_time": "2024-04-18T10:36:17.143078Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "a = tokenizer('hello this is a test',\n",
    "         truncation=True,\n",
    "         padding='max_length',\n",
    "         max_length=512,\n",
    "         return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1f09d5233be3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:22.222134Z",
     "start_time": "2024-04-18T10:36:18.117859Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from gpt1 import GPT1Dataset\n",
    "\n",
    "semantic_items = [item[0][4] + ' ' + item[0][5] + ' ' + item[0][6] for item in train_data]\n",
    "input_ids, attention_mask = [], []\n",
    "for description in semantic_items:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    # input_ids.append(encoding['input_ids'].squeeze())\n",
    "    # attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    input_ids.append(encoding['input_ids'][0])\n",
    "    attention_mask.append(encoding['attention_mask'][0])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "input_ids = torch.stack(input_ids).to(device)\n",
    "attention_mask = torch.stack(attention_mask).to(device)\n",
    "\n",
    "labels = [float(target) for _, target in train_data]\n",
    "\n",
    "train_dataset = GPT1Dataset(input_ids, attention_mask, train_cat_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc429ee792143ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:25.734327Z",
     "start_time": "2024-04-18T10:36:23.231319Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "semantic_items = [item[0][4] + ' ' + item[0][5] + ' ' + item[0][6] for item in val_data]\n",
    "input_ids, attention_mask = [], []\n",
    "for description in semantic_items:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    # input_ids.append(encoding['input_ids'].squeeze())\n",
    "    # attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    input_ids.append(encoding['input_ids'][0])\n",
    "    attention_mask.append(encoding['attention_mask'][0])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "input_ids = torch.stack(input_ids).to(device)\n",
    "attention_mask = torch.stack(attention_mask).to(device)\n",
    "\n",
    "labels = [float(target) for _, target in val_data]\n",
    "\n",
    "val_dataset = GPT1Dataset(input_ids, attention_mask, val_cat_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49fa8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_items = [item[0][4] + ' ' + item[0][5] + ' ' + item[0][6] for item in test_data]\n",
    "input_ids, attention_mask = [], []\n",
    "for description in semantic_items:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    # input_ids.append(encoding['input_ids'].squeeze())\n",
    "    # attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    input_ids.append(encoding['input_ids'][0])\n",
    "    attention_mask.append(encoding['attention_mask'][0])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "input_ids = torch.stack(input_ids).to(device)\n",
    "attention_mask = torch.stack(attention_mask).to(device)\n",
    "\n",
    "labels = [float(target) for _, target in test_data]\n",
    "\n",
    "test_dataset = GPT1Dataset(input_ids, attention_mask, test_cat_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "206cac4e80e854b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:27.521643Z",
     "start_time": "2024-04-18T10:36:27.505099Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 1035,   268,  7471, 22429, 17397,  2192,  9189,  1890,  7613, 17397,\n",
       "          2192,   960,   495,  5230,  6123, 30880,  1874,  3594,   500,  2759,\n",
       "          1801,  1755,   896,   541, 39955,   720,  6307,   485,  5513,   292,\n",
       "          1048,   485,   284,    24,    20,  3594,   498,  7049, 26459,   485,\n",
       "           750,    20,  2596,  6144, 13244, 27277, 14814,  1490,  2088, 18773,\n",
       "         22136, 22720, 19231,  3388,   641,  7127,   531, 17397,  2192,  9189,\n",
       "           485,  8052,   481,  3866,   488, 12649, 10940,   498,   622, 17397,\n",
       "          5480,   616,  6118, 18993,   246,  7087,   498, 22754,   488, 20860,\n",
       "         12993,  9232,   504,  7739,   649,  3669, 10760,  3987,   488, 10051,\n",
       "           481, 17397,  2192,  9189,  9862,   246, 10875,  6118,   500, 15306,\n",
       "         29599,   556, 22746,   488,  7174, 11866,  4640, 17397, 16015,   488,\n",
       "         37973,   622,  9425, 39375,   488,  3186, 14447,  4101, 14234,   488,\n",
       "         16370,  1801, 12289,   488, 18821, 11553, 20011,   488, 25884,  5599,\n",
       "         13110,   488, 21329,   562,  2006,  2963,  7427,    14, 21994,   485,\n",
       "         17397, 33450,   488,  9075,  4258,   744,   887,  3791,   660, 15008,\n",
       "           488, 19331,  1285,  2003,   582,   506,  3054,   783, 20011,   488,\n",
       "         25884,   485, 25955, 10653,  3282,   488, 17397,  5480,  7794,  2059,\n",
       "           496,  3113,  8052, 29599,   556,  3935,   488,  5670, 22746, 37630,\n",
       "         25884,   488, 20011,   557,  1194,   585,    10,  1486,   557,   481,\n",
       "         11942, 11270,   485,   481, 11807,  8822, 16762,   485,  1973, 17355,\n",
       "          5095,   582,  1285,   640,  8852,   488, 22921,   562,   246, 37081,\n",
       "           550,   684,   716,   494, 12370,  5758,    14,   576,  1086,   541,\n",
       "           589, 22754, 12993,  7105,   485,   494, 12370,   550,   684,   716,\n",
       "          5736,  3550,  1600,   488,  5073,  1136, 16676,   559,   787,  3410,\n",
       "         10286, 15288,  6218,  1427, 28708, 17934,    21,   262,   669,  1194,\n",
       "           585,  2704, 12370,  5736, 29844,  3590,   291,   779,    11, 16262,\n",
       "          6218,  1427, 26646,   556,   589, 36491,  2894,   669,  1388,   488,\n",
       "          6675,   557,   246, 11270,   645,   481, 11807,  8822, 16762,   544,\n",
       "          1224, 18259,  9163, 12810,   491,  1098, 22312,    14,  2074,  8932,\n",
       "           488,  1136,  3191,  6074,   556,   531, 14028,   504, 23404,   488,\n",
       "         15959,  7731,   496,  1801,   582,   506,  1397,    20, 14502,  6837,\n",
       "           488,  2670,    24,   663,  3354,   498, 17397, 20011,   488,  1993,\n",
       "           814,     5, 21782,  2145,   940,  3354,   498,     4,  4625,   488,\n",
       "           481,  3371,   485,  3079,   783, 15123, 25739,  2269,  5072, 12084,\n",
       "           488,  1881,    20, 14502, 22312,   540,  3258,   485,  8186, 18904,\n",
       "         33836,   488, 30011,   501,   502,    15, 13132,  3354,   498,  5681,\n",
       "          1448,  1066, 36713,  7174,   488, 16412,  7427,   582,   506,    12,\n",
       "         15141,   556,  6239,  2022,  1876,  6902,   522,  7105, 15532, 15490,\n",
       "          1448, 12773, 38222,  6937,  5486,   500, 29072,   498,   246,  6937,\n",
       "          5895,   522,   725,  1218,   498, 11445,  4010,  1345, 23339,  1218,\n",
       "           498,  3282,  2029, 23721, 11149, 13056,    34,  1818,   938,  1996,\n",
       "           556,   531,     4,  4625,  3424,   556,  5794,  5259,  3282,  3136,\n",
       "           246,  7926,  2545,    14,  1860,   667,    16,   679,  4240,  8878,\n",
       "          6890,   491,  1035,   268,   606,  1441,   500,  1457,   246,  4166,\n",
       "           566,  1800,   491,   246,   720,  1035,   268, 37242,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0], device='cuda:0'),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
       " 'categorical_features': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0.], device='cuda:0'),\n",
       " 'labels': 57600.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6025166c04034571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:34.993781Z",
     "start_time": "2024-04-18T10:36:33.099022Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Loss: 8868547584.0 Train mae 81166.2375, Validation mae 85536.165\n",
      "Iter 100: Loss: 6232518144.0 Train mae 45299.144375, Validation mae 50172.601875\n",
      "Iter 150: Loss: 1249763200.0 Train mae 35345.2175, Validation mae 44598.883125\n",
      "Iter 200: Loss: 1925807744.0 Train mae 36404.234375, Validation mae 45565.256875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT1(\u001b[38;5;28mlen\u001b[39m(vocab_type) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_industry) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_state) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_level))\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\src\\models\\gpt1.py:141\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data, val_data, learning_rate, batch_size, eval_batch_size, num_epochs, plot_every, plot)\u001b[0m\n\u001b[0;32m    138\u001b[0m label \u001b[38;5;241m=\u001b[39m label\n\u001b[0;32m    140\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 141\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    143\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, label\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\src\\models\\gpt1.py:46\u001b[0m, in \u001b[0;36mGPT1.forward\u001b[1;34m(self, input_ids, attention_mask, categorical_features)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, categorical_features):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Process textual input through GPT\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     text_features \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Use the last token's representation\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Concatenate text features with categorical features\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\transformers\\models\\openai\\modeling_openai.py:506\u001b[0m, in \u001b[0;36mOpenAIGPTModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    504\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m--> 506\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\transformers\\models\\openai\\modeling_openai.py:254\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 254\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m     a \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    262\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x \u001b[38;5;241m+\u001b[39m a)\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\transformers\\models\\openai\\modeling_openai.py:218\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    215\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(key, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    216\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_heads(value)\n\u001b[1;32m--> 218\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m a \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    221\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge_heads(a)\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\transformers\\models\\openai\\modeling_openai.py:186\u001b[0m, in \u001b[0;36mAttention._attn\u001b[1;34m(self, q, k, v, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[1;32m--> 186\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_dropout(w)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1828\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1824\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[1;32m-> 1828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m   1829\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply a softmax function.\u001b[39;00m\n\u001b[0;32m   1830\u001b[0m \n\u001b[0;32m   1831\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1851\u001b[0m \n\u001b[0;32m   1852\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_trace_dispatch_regular.py:326\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# ENDIF\u001b[39;00m\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, frame, event, arg):\n\u001b[0;32m    327\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m''' This is the callback used when we enter some context in the debugger.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m        We also decorate the thread we are in with info about the debugging.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;124;03m            This is the global debugger (this method should actually be added as a method to it).\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m        '''\u001b[39;00m\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;66;03m# IFDEF CYTHON\u001b[39;00m\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;66;03m# cdef str filename;\u001b[39;00m\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;66;03m# cdef str base;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;66;03m# DEBUG = 'code_to_debug' in frame.f_code.co_filename\u001b[39;00m\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;66;03m# if DEBUG: print('ENTER: trace_dispatch: %s %s %s %s' % (frame.f_code.co_filename, frame.f_lineno, event, frame.f_code.co_name))\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gpt1 import GPT1\n",
    "from gpt1 import train_model\n",
    "\"\"\"remember to use gpu i.e. model.to(device) device = 'cuda' \"\"\"\n",
    "\n",
    "model = GPT1(len(vocab_type) + len(vocab_industry) + len(vocab_state) + len(vocab_level))\n",
    "model.to(device)\n",
    "\n",
    "train_model(model, train_data=train_dataset, val_data=val_dataset, batch_size=20, eval_batch_size=20, num_epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "583edebc5ddb9524",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6db2a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import OpenAIGPTModel\n",
    "class GPT1_3LL(nn.Module):\n",
    "    def __init__(self, num_categorical_features):\n",
    "        super(GPT1_3LL, self).__init__()\n",
    "\n",
    "        self.gpt = OpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(self.gpt.config.hidden_size + num_categorical_features, 500)\n",
    "        self.fc2 = nn.Linear(500, 250)\n",
    "        self.fc3 = nn.Linear(250, 100)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.output = nn.Linear(100, 1)  # Output layer for salary prediction\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, categorical_features):\n",
    "        # Process textual input through GPT\n",
    "        outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        text_features = outputs.last_hidden_state[:, -1, :]  # Use the last token's representation\n",
    "\n",
    "        # Concatenate text features with categorical features\n",
    "        combined_features = torch.cat((text_features, categorical_features), dim=1)\n",
    "        combined_features = self.fc1(combined_features)\n",
    "        x = self.relu(combined_features)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51af747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Loss: 2304969216.0 Train mae 43995.17275, Validation mae 133732.32808333333\n",
      "Iter 100: Loss: 2342991872.0 Train mae 40187.832708333335, Validation mae 129563.01847916667\n",
      "Iter 150: Loss: 2517573632.0 Train mae 41184.2993125, Validation mae 131211.11129166666\n",
      "Iter 200: Loss: 1892202112.0 Train mae 46290.084375, Validation mae 136913.785375\n",
      "Iter 250: Loss: 9334715392.0 Train mae 83573.45725, Validation mae 173041.46491666668\n",
      "Iter 300: Loss: 6555491840.0 Train mae 61665.673625, Validation mae 151432.96091666666\n",
      "Iter 350: Loss: 621434112.0 Train mae 39524.65902083334, Validation mae 129395.229\n",
      "Iter 400: Loss: 2730205952.0 Train mae 40443.1231875, Validation mae 130230.65254166667\n",
      "Iter 450: Loss: 1720825088.0 Train mae 39561.436604166665, Validation mae 129201.3484375\n",
      "Iter 500: Loss: 923200960.0 Train mae 39539.576166666666, Validation mae 129019.21333333333\n",
      "Iter 550: Loss: 4472351232.0 Train mae 41313.34720833333, Validation mae 131163.35727083334\n",
      "Iter 600: Loss: 7872652800.0 Train mae 61647.78083333333, Validation mae 152569.209875\n",
      "Iter 650: Loss: 1108373376.0 Train mae 39468.665375, Validation mae 128993.05814583333\n",
      "Iter 700: Loss: 1315568256.0 Train mae 39891.2831875, Validation mae 129459.21258333333\n",
      "Iter 750: Loss: 4816104960.0 Train mae 40938.219875, Validation mae 130372.85383333333\n",
      "Iter 800: Loss: 1053679744.0 Train mae 40631.544708333335, Validation mae 130185.52445833334\n",
      "Iter 850: Loss: 1407031808.0 Train mae 41263.54341666667, Validation mae 131247.54110416668\n",
      "Iter 900: Loss: 3136064000.0 Train mae 40289.9415625, Validation mae 129900.094\n",
      "Iter 950: Loss: 1241259904.0 Train mae 40552.68645833333, Validation mae 130131.0616875\n",
      "Iter 1000: Loss: 2315398400.0 Train mae 40278.2254375, Validation mae 130079.86825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model2 \u001b[38;5;241m=\u001b[39m GPT1_3LL(\u001b[38;5;28mlen\u001b[39m(vocab_type) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_industry) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_state) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab_level))\n\u001b[0;32m      2\u001b[0m model2\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\src\\models\\gpt1.py:144\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_data, val_data, learning_rate, batch_size, eval_batch_size, num_epochs, plot_every, plot)\u001b[0m\n\u001b[0;32m    142\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    143\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, label\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m--> 144\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    147\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2 = GPT1_3LL(len(vocab_type) + len(vocab_industry) + len(vocab_state) + len(vocab_level))\n",
    "model2.to(device)\n",
    "\n",
    "train_model(model2, train_data=train_dataset, val_data=val_dataset, batch_size=20, eval_batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f04ca412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86231.99638125\n"
     ]
    }
   ],
   "source": [
    "from gpt1 import _collate_batch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def calculate_mae(model, dataloader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the mean absolute error for a model over a given dataloader.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        dataloader (DataLoader): The DataLoader containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean absolute error of the model.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, categorical_features, labels in dataloader:\n",
    "            outputs = model(input_ids, attention_mask, categorical_features)\n",
    "            outputs = outputs.squeeze()  # Adjust shape if necessary\n",
    "            distances = torch.abs(labels - outputs)\n",
    "            total_distance += distances.sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    return total_distance / total_count\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False, collate_fn=_collate_batch)\n",
    "print(calculate_mae(model2, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4d4f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73331.4451921875\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=False, collate_fn=_collate_batch)\n",
    "print(calculate_mae(model2, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92211f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38531.93207586786\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False, collate_fn=_collate_batch)\n",
    "print(calculate_mae(model2, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5896a7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
