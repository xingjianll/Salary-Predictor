{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:08:07.309599Z",
     "start_time": "2024-04-17T22:08:02.606597Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.models.utils import read_csv_data, clean_location\n",
    "\n",
    "data = read_csv_data(\"../data/processed_job_postings_large.csv\", \n",
    "                     [\"industry\", \"work_type\", \"location\", \"formatted_experience_level\",\n",
    "                      \"name\", \"cleaned_title\", \"cleaned_description\"],\n",
    "                     \"standardized_annual_salary\")\n",
    "data = clean_location(data, 2)\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:20000]\n",
    "val_data = data[20000:30000]\n",
    "test_data = data[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "7"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.utils import build_column_vocabulary\n",
    "\n",
    "vocab_sector = build_column_vocabulary(train_data, 0)\n",
    "vocab_type = build_column_vocabulary(train_data, 1)\n",
    "vocab_state = build_column_vocabulary(train_data, 2)\n",
    "vocab_level = build_column_vocabulary(train_data, 3)\n",
    "len(vocab_type)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:08:07.729291Z",
     "start_time": "2024-04-17T22:08:07.312200Z"
    }
   },
   "id": "72692612c9cf88a0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from src.models.utils import convert_to_one_hot\n",
    "train_cat_features = convert_to_one_hot(train_data, \n",
    "                                  [(0, vocab_sector),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "val_cat_features = convert_to_one_hot(val_data, \n",
    "                                  [(0, vocab_sector),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:08:16.647245Z",
     "start_time": "2024-04-17T22:08:07.731693Z"
    }
   },
   "id": "a4fd8830c588f794",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "a = tokenizer('hello this is a test',\n",
    "         truncation=True,\n",
    "         padding='max_length',\n",
    "         max_length=512,\n",
    "         return_tensors=\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:08:17.618947Z",
     "start_time": "2024-04-17T22:08:16.649579Z"
    }
   },
   "id": "9d1c3502cb523762",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from src.models.gpt1 import GPT1Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "descriptions = [item[0][6] for item in train_data]\n",
    "input_ids, attention_mask = [], []\n",
    "for description in descriptions:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    input_ids.append(encoding['input_ids'].squeeze())\n",
    "    attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    \n",
    "labels = [float(target) for _, target in train_data]\n",
    "\n",
    "train_dataset = GPT1Dataset(input_ids, attention_mask, train_cat_features, labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:09:07.581054Z",
     "start_time": "2024-04-17T22:08:17.620608Z"
    }
   },
   "id": "1f09d5233be3f3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([ 6303,  5253, 23263,   640,  5154,   562,  4387,   488, 33243, 12810,\n           485,  8052,   600,   640,  7826,   500,   246, 32968,  6425,   488,\n          2029, 14121,  1996,  7404,   481,  5253,  6844,   485,  2236,   488,\n         12561,   746,  5253,  9535,  6672,  8153,  2694,  8017,  6112,   488,\n          1178, 40443, 20369,  5655,   481,  2179,  9269,  6875,  2906,   555,\n          9514,  6875,   544,   566,   498,   481,  1495, 26789,  3304, 31375,\n           500,   481,  6391,  7876,   600,   640,  1081,   562,   531,  5007,\n          6303,  5253,  6844,   485,  3351,   754,  3170,  2855,   556,   531,\n          6636,  6271, 39135,   745,   562,  4804, 10361,   488,  2429,   616,\n          7391,   500, 35627,   562,   754,  6589,  1463,   488, 10998,  8426,\n         18298, 19231,  3388,  1510,  1383, 12602, 14152, 28530,   488,  6190,\n           519,  2102,  5142,  7730,   517,  1801, 22537,  5253, 11675, 16680,\n           488,  3675,   485,  9945,   720,  4317, 19005, 23006, 20011,   562,\n         36060,  5253,  3669, 10760, 21403,   488, 14799,  7242,   498,  5467,\n          9535,   485,  4640,  6126,  1227,   498, 12622,     3, 39994,  9807,\n           556,  1840, 31375,   488, 39733, 15450,   498,  1840, 31375,   525,\n          9036,  7410,  5847,   625, 12700,   556,  5253,  3997,   485, 10424,\n          1129,  2236,   488,   485, 27221,  9315, 14234,   488,  9777,   498,\n         19975,    15, 33414,   488, 19048,  9171,   498,  5253, 11553,   485,\n          8052,  5253,  4969,  1633,   504,  6816,   488,  2029, 27712,   545,\n          7678,     3, 37209,  6858, 15270, 19814,  6816,   485,  3937,   488,\n         13878,   481,  7049,  5253,  4986,  5001, 12811,   996,  7788,  7268,\n          3572,   702,  5253, 11553,   488, 13458, 14343, 22921,   522,  3675,\n           557, 15490,   970,   572,     3,  1917,   522, 37209,  5253,  7268,\n           562, 12128,  6875,   522,  3686, 16796, 12700,   556,  5253, 11553,\n           485,  6159, 15123,  5553,   488,   485,  9093,  1881,    20,   582,\n           506,   710,   552,   280,  1218,   498,  8426,  4010,    25,   859,\n            17,   731,  5614,   500,  6589,  1463, 10998,   625,  4710,  3388,\n          2074, 20316,   488,  3857,  8932, 22312,   582,   506,    12,  7169,\n           500,  6239,  2022,  1876,  5097,     5,   481,  3371,   485,  1129,\n           500,   246,  1708,  7948,  1129,  8412,  1448, 25719,   485,  1129,\n         29561,   488,   557,  1250,   498,   246, 18298,   481,  3183,   485,\n          4097,   754,  5784, 18320, 27187,   481,  3183,   485,  2413,   481,\n          9638,   498,   246,  6711, 30656,  3304,  9648,  1525,   481,  3183,\n           485, 13259,   803,   498,   481,  4066,  5253,  4510,   486,    14,\n          2074,  3921,  5319,   556,  6875,  2934, 11865,    14,  1890,  1067,\n           486,  3888, 16234,   498,    50, 12354, 12354,    53,    48,  7397,\n         11343,   504,  4010,  1345,  8912,  5945, 14402,  5295,   536,   501,\n          2600,  3999,   502,    15,  1708, 10103,  5784,  4969, 14890,  1463,\n         28242,  1047,   791,  2600,   952,   882, 21394,  1463,  6558,  8954,\n          1575,  4130,   539,  1019,   486,  5147,    15, 33538,   677,  1426,\n         24744,    39,   265,  2236,   556,  2600, 31190, 10114,  6074,   488,\n          3282,   280,  1218,   498,  8426,  3282,  2074,  5614,   500,  6589,\n          1463, 10998,  8426,  2074, 20316,   488,  3857,  8932,  6074, 32278,\n           500,  6239,  2022,  1876,  6902,   481,  3371,   485,  1129,   500,\n           246,  1708,  7948,  1129,  9961,  3371,   485,  1129, 29561,   488,\n           557,  1250,   498,   246,  2855,   481,  3183,   485,  4097,   754,\n          5784, 22048,   481,  3183,   485,  2413,   481,  9638,   498,   246,\n          6711, 30656,  3304, 24948,   481,  3183,   485, 13259,   803,   498,\n           481,  4066,  5253, 23263,  2074,  3921,  5319,   556,  6875,  9807,\n           787,   249,  1056,   595, 25692, 22023,   504,   481,  9351,   498,\n          4115,  3184]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1]),\n 'categorical_features': tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n         0.]),\n 'labels': tensor(80000.)}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:09:07.594531Z",
     "start_time": "2024-04-17T22:09:07.582681Z"
    }
   },
   "id": "206cac4e80e854b6",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/PycharmProjects/Salary-Predictor/.venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# copied from https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# \n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "# \n",
    "# Check GPU compatibility with bfloat16\n",
    "# if compute_dtype == torch.float16 and use_4bit:\n",
    "#     major, _ = torch.cuda.get_device_capability()\n",
    "#     if major >= 8:\n",
    "#         print(\"=\" * 80)\n",
    "#         print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "#         print(\"=\" * 80)\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    fan_in_fan_out=True\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=[\"tensorboard\"]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:09:07.752072Z",
     "start_time": "2024-04-17T22:09:07.597726Z"
    }
   },
   "id": "8accdb120e1c74a6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT1(\n",
      "  (gpt): OpenAIGPTModel(\n",
      "    (tokens_embed): Embedding(40478, 768)\n",
      "    (positions_embed): Embedding(512, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x Block(\n",
      "        (attn): Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=1075, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from src.models.gpt1 import GPT1\n",
    "\n",
    "model = GPT1(len(vocab_type) + len(vocab_sector) + len(vocab_state) + len(vocab_level))\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:09:09.745332Z",
     "start_time": "2024-04-17T22:09:07.753750Z"
    }
   },
   "id": "6025166c04034571",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/PycharmProjects/Salary-Predictor/.venv/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.models.gpt1 import collate_batch\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_batch,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:09:12.470121Z",
     "start_time": "2024-04-17T22:09:09.748696Z"
    }
   },
   "id": "e6965d2b76981fe9",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [] doesn't match the broadcast shape [1]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Salary-Predictor/.venv/lib/python3.11/site-packages/transformers/trainer.py:1780\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1778\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1780\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1781\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1782\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1783\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1784\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1785\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/Salary-Predictor/.venv/lib/python3.11/site-packages/transformers/trainer.py:2132\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2128\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tr_loss\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m!=\u001B[39m tr_loss_step\u001B[38;5;241m.\u001B[39mdevice:\n\u001B[1;32m   2129\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2130\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCalculated loss must be on the original device: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtr_loss\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m but device in use is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtr_loss_step\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2131\u001B[0m         )\n\u001B[0;32m-> 2132\u001B[0m     \u001B[43mtr_loss\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtr_loss_step\u001B[49m\n\u001B[1;32m   2134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_flos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfloating_point_ops(inputs))\n\u001B[1;32m   2136\u001B[0m is_last_step_and_steps_less_than_grad_acc \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2137\u001B[0m     steps_in_epoch \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m args\u001B[38;5;241m.\u001B[39mgradient_accumulation_steps \u001B[38;5;129;01mand\u001B[39;00m (step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m==\u001B[39m steps_in_epoch\n\u001B[1;32m   2138\u001B[0m )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: output with shape [] doesn't match the broadcast shape [1]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:09:33.646716Z",
     "start_time": "2024-04-17T22:09:12.473546Z"
    }
   },
   "id": "3c58a655fe9f3d6c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T22:09:33.649999Z",
     "start_time": "2024-04-17T22:09:33.649649Z"
    }
   },
   "id": "47ca553cbcc0eb2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
