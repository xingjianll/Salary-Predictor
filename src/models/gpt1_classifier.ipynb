{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:11.225642Z",
     "start_time": "2024-04-18T10:36:09.249303Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import read_csv_data, clean_location\n",
    "\n",
    "data = read_csv_data(\"../data/processed_job_postings_large_noNA.csv\", \n",
    "                     [\"industry\", \"work_type\", \"location\", \"formatted_experience_level\",\n",
    "                      \"name\", \"cleaned_title\", \"cleaned_description\", \"title_emb\"],\n",
    "                     \"salary_level\")\n",
    "data = clean_location(data, 2)\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "train_data = data[:10000]\n",
    "val_data = data[10000:13000]\n",
    "test_data = data[13000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72692612c9cf88a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:14.703238Z",
     "start_time": "2024-04-18T10:36:13.991539Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import build_column_vocabulary\n",
    "\n",
    "vocab_industry = build_column_vocabulary(train_data, 0)\n",
    "vocab_type = build_column_vocabulary(train_data, 1)\n",
    "vocab_state = build_column_vocabulary(train_data, 2)\n",
    "vocab_level = build_column_vocabulary(train_data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054c74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "targets = [data[i][1] for i in range(len(data))]\n",
    "\n",
    "labels = ['10K-'] + [f\"{i}K - {i + 10}K\" for i in range(10, 150, 10)] + ['160K+']\n",
    "label_to_int = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "targets = list(map(lambda x: label_to_int[x], targets))\n",
    "\n",
    "targets = torch.tensor(targets)\n",
    "\n",
    "train_targets = targets[:10000]\n",
    "val_targets = targets[10000:13000]\n",
    "test_targets = targets[13000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a4bc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4fd8830c588f794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:16.080469Z",
     "start_time": "2024-04-18T10:36:15.334263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import convert_to_one_hot\n",
    "import torch\n",
    "\n",
    "train_cat_features = convert_to_one_hot(train_data, \n",
    "                                  [(0, vocab_industry),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "val_cat_features = convert_to_one_hot(val_data, \n",
    "                                  [(0, vocab_industry),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "test_cat_features = convert_to_one_hot(test_data, \n",
    "                                  [(0, vocab_industry),\n",
    "                                   (1, vocab_type),\n",
    "                                   (2, vocab_state),\n",
    "                                   (3, vocab_level)])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "train_cat_features = torch.stack(train_cat_features).to(device)\n",
    "val_cat_features = torch.stack(val_cat_features).to(device)\n",
    "test_cat_features = torch.stack(test_cat_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1c3502cb523762",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:17.354505Z",
     "start_time": "2024-04-18T10:36:17.143078Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "a = tokenizer('hello this is a test',\n",
    "         truncation=True,\n",
    "         padding='max_length',\n",
    "         max_length=512,\n",
    "         return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f09d5233be3f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:22.222134Z",
     "start_time": "2024-04-18T10:36:18.117859Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gpt1 import GPT1Dataset\n",
    "\n",
    "semantic_items = [item[0][4] + ' ' + item[0][5] + ' ' + item[0][6] for item in train_data]\n",
    "input_ids_train, attention_mask_train = [], []\n",
    "for description in semantic_items:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    # input_ids.append(encoding['input_ids'].squeeze())\n",
    "    # attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    input_ids_train.append(encoding['input_ids'][0])\n",
    "    attention_mask_train.append(encoding['attention_mask'][0])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "input_ids_train = torch.stack(input_ids_train).to(device)\n",
    "attention_mask_train = torch.stack(attention_mask_train).to(device)\n",
    "\n",
    "labels = [target for target in train_targets]\n",
    "\n",
    "train_dataset = GPT1Dataset(input_ids_train, attention_mask_train, train_cat_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc429ee792143ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:25.734327Z",
     "start_time": "2024-04-18T10:36:23.231319Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "semantic_items = [item[0][4] + ' ' + item[0][5] + ' ' + item[0][6] for item in val_data]\n",
    "input_ids_val, attention_mask_val = [], []\n",
    "for description in semantic_items:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    # input_ids.append(encoding['input_ids'].squeeze())\n",
    "    # attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    input_ids_val.append(encoding['input_ids'][0])\n",
    "    attention_mask_val.append(encoding['attention_mask'][0])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "input_ids_val = torch.stack(input_ids_val).to(device)\n",
    "attention_mask_val = torch.stack(attention_mask_val).to(device)\n",
    "\n",
    "labels = [target for target in val_targets]\n",
    "\n",
    "val_dataset = GPT1Dataset(input_ids_val, attention_mask_val, val_cat_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49fa8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_items = [item[0][4] + ' ' + item[0][5] + ' ' + item[0][6] for item in test_data]\n",
    "input_ids_test, attention_mask_test = [], []\n",
    "for description in semantic_items:\n",
    "    encoding = tokenizer(description,\n",
    "                         truncation=True,\n",
    "                         padding='max_length',\n",
    "                         max_length=512,\n",
    "                         return_tensors=\"pt\")\n",
    "    # input_ids.append(encoding['input_ids'].squeeze())\n",
    "    # attention_mask.append(encoding['attention_mask'].squeeze())\n",
    "    input_ids_test.append(encoding['input_ids'][0])\n",
    "    attention_mask_test.append(encoding['attention_mask'][0])\n",
    "\n",
    "# Convert Lists to Tensors\n",
    "input_ids_test = torch.stack(input_ids_test).to(device)\n",
    "attention_mask_test = torch.stack(attention_mask_test).to(device)\n",
    "\n",
    "labels = [target for target in test_targets]\n",
    "\n",
    "test_dataset = GPT1Dataset(input_ids_test, attention_mask_test, test_cat_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6025166c04034571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T10:36:34.993781Z",
     "start_time": "2024-04-18T10:36:33.099022Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Loss: 2.6228525638580322 Train Acc: 0.12866666666666668, Validation Acc: 0.146\n",
      "Iter 100: Loss: 2.6336631774902344 Train Acc: 0.136, Validation Acc: 0.16266666666666665\n",
      "Iter 150: Loss: 2.746643543243408 Train Acc: 0.13866666666666666, Validation Acc: 0.156\n",
      "Iter 200: Loss: 2.5612542629241943 Train Acc: 0.13933333333333334, Validation Acc: 0.158\n",
      "Iter 250: Loss: 2.3961730003356934 Train Acc: 0.14066666666666666, Validation Acc: 0.15866666666666668\n",
      "Iter 300: Loss: 2.7208762168884277 Train Acc: 0.16666666666666666, Validation Acc: 0.17133333333333334\n",
      "Iter 350: Loss: 2.548516035079956 Train Acc: 0.182, Validation Acc: 0.17333333333333334\n",
      "Iter 400: Loss: 2.681497097015381 Train Acc: 0.13866666666666666, Validation Acc: 0.158\n",
      "Iter 450: Loss: 2.5745849609375 Train Acc: 0.16266666666666665, Validation Acc: 0.18333333333333332\n",
      "Iter 500: Loss: 2.702854633331299 Train Acc: 0.19733333333333333, Validation Acc: 0.18733333333333332\n",
      "Iter 550: Loss: 2.5043535232543945 Train Acc: 0.20733333333333334, Validation Acc: 0.19933333333333333\n",
      "Iter 600: Loss: 2.455355167388916 Train Acc: 0.20333333333333334, Validation Acc: 0.208\n",
      "Iter 650: Loss: 2.569228410720825 Train Acc: 0.224, Validation Acc: 0.21666666666666667\n",
      "Iter 700: Loss: 2.1235947608947754 Train Acc: 0.21666666666666667, Validation Acc: 0.2\n",
      "Iter 750: Loss: 2.2413086891174316 Train Acc: 0.222, Validation Acc: 0.21\n",
      "Iter 800: Loss: 2.2348971366882324 Train Acc: 0.24533333333333332, Validation Acc: 0.226\n",
      "Iter 850: Loss: 2.5520358085632324 Train Acc: 0.236, Validation Acc: 0.23666666666666666\n",
      "Iter 900: Loss: 1.9183257818222046 Train Acc: 0.2733333333333333, Validation Acc: 0.25\n",
      "Iter 950: Loss: 2.4787240028381348 Train Acc: 0.25533333333333336, Validation Acc: 0.24733333333333332\n",
      "Iter 1000: Loss: 2.3184654712677 Train Acc: 0.26466666666666666, Validation Acc: 0.24733333333333332\n",
      "Iter 1050: Loss: 2.002213478088379 Train Acc: 0.26266666666666666, Validation Acc: 0.25066666666666665\n",
      "Iter 1100: Loss: 2.0923588275909424 Train Acc: 0.25866666666666666, Validation Acc: 0.26\n",
      "Iter 1150: Loss: 2.4447343349456787 Train Acc: 0.27, Validation Acc: 0.25733333333333336\n",
      "Iter 1200: Loss: 2.1544132232666016 Train Acc: 0.23266666666666666, Validation Acc: 0.21866666666666668\n",
      "Iter 1250: Loss: 1.956989049911499 Train Acc: 0.28733333333333333, Validation Acc: 0.268\n",
      "Iter 1300: Loss: 2.242849826812744 Train Acc: 0.30866666666666664, Validation Acc: 0.2613333333333333\n",
      "Iter 1350: Loss: 2.6099319458007812 Train Acc: 0.30733333333333335, Validation Acc: 0.2753333333333333\n",
      "Iter 1400: Loss: 2.105713367462158 Train Acc: 0.3006666666666667, Validation Acc: 0.284\n",
      "Iter 1450: Loss: 2.242310047149658 Train Acc: 0.3253333333333333, Validation Acc: 0.278\n",
      "Iter 1500: Loss: 2.2876734733581543 Train Acc: 0.286, Validation Acc: 0.25333333333333335\n",
      "Iter 1550: Loss: 2.497004985809326 Train Acc: 0.31466666666666665, Validation Acc: 0.26866666666666666\n",
      "Iter 1600: Loss: 1.825055718421936 Train Acc: 0.316, Validation Acc: 0.2853333333333333\n",
      "Iter 1650: Loss: 1.9706884622573853 Train Acc: 0.31066666666666665, Validation Acc: 0.2713333333333333\n",
      "Iter 1700: Loss: 2.220503091812134 Train Acc: 0.3333333333333333, Validation Acc: 0.284\n",
      "Iter 1750: Loss: 1.6167223453521729 Train Acc: 0.31733333333333336, Validation Acc: 0.2966666666666667\n",
      "Iter 1800: Loss: 2.26655912399292 Train Acc: 0.30466666666666664, Validation Acc: 0.2813333333333333\n",
      "Iter 1850: Loss: 1.7669570446014404 Train Acc: 0.33066666666666666, Validation Acc: 0.29733333333333334\n",
      "Iter 1900: Loss: 1.7922544479370117 Train Acc: 0.334, Validation Acc: 0.2806666666666667\n",
      "Iter 1950: Loss: 1.8800830841064453 Train Acc: 0.31933333333333336, Validation Acc: 0.2793333333333333\n",
      "Iter 2000: Loss: 2.2305004596710205 Train Acc: 0.35533333333333333, Validation Acc: 0.2986666666666667\n",
      "Iter 2050: Loss: 1.7931562662124634 Train Acc: 0.3546666666666667, Validation Acc: 0.30533333333333335\n",
      "Iter 2100: Loss: 1.8707752227783203 Train Acc: 0.376, Validation Acc: 0.3\n",
      "Iter 2150: Loss: 1.9723179340362549 Train Acc: 0.35933333333333334, Validation Acc: 0.30133333333333334\n",
      "Iter 2200: Loss: 2.156419515609741 Train Acc: 0.36933333333333335, Validation Acc: 0.314\n",
      "Iter 2250: Loss: 1.961925745010376 Train Acc: 0.37733333333333335, Validation Acc: 0.29933333333333334\n",
      "Iter 2300: Loss: 2.3639883995056152 Train Acc: 0.41933333333333334, Validation Acc: 0.32266666666666666\n",
      "Iter 2350: Loss: 2.302638530731201 Train Acc: 0.398, Validation Acc: 0.3313333333333333\n",
      "Iter 2400: Loss: 1.792425513267517 Train Acc: 0.3933333333333333, Validation Acc: 0.328\n",
      "Iter 2450: Loss: 1.6980721950531006 Train Acc: 0.4146666666666667, Validation Acc: 0.32866666666666666\n",
      "Iter 2500: Loss: 1.7624340057373047 Train Acc: 0.35133333333333333, Validation Acc: 0.30133333333333334\n",
      "Iter 2550: Loss: 1.9870965480804443 Train Acc: 0.41133333333333333, Validation Acc: 0.3293333333333333\n",
      "Iter 2600: Loss: 1.9871785640716553 Train Acc: 0.32866666666666666, Validation Acc: 0.26666666666666666\n",
      "Iter 2650: Loss: 1.889906883239746 Train Acc: 0.42933333333333334, Validation Acc: 0.31333333333333335\n",
      "Iter 2700: Loss: 1.66092050075531 Train Acc: 0.388, Validation Acc: 0.32133333333333336\n",
      "Iter 2750: Loss: 1.910556435585022 Train Acc: 0.42866666666666664, Validation Acc: 0.33\n",
      "Iter 2800: Loss: 1.8230371475219727 Train Acc: 0.426, Validation Acc: 0.34933333333333333\n",
      "Iter 2850: Loss: 3.3784384727478027 Train Acc: 0.11133333333333334, Validation Acc: 0.10133333333333333\n",
      "Iter 2900: Loss: 2.487431049346924 Train Acc: 0.14133333333333334, Validation Acc: 0.16\n",
      "Iter 2950: Loss: 2.621814250946045 Train Acc: 0.14133333333333334, Validation Acc: 0.15866666666666668\n",
      "Iter 3000: Loss: 2.495378255844116 Train Acc: 0.13866666666666666, Validation Acc: 0.158\n",
      "Iter 3050: Loss: 2.5648343563079834 Train Acc: 0.20466666666666666, Validation Acc: 0.19466666666666665\n",
      "Iter 3100: Loss: 2.402818441390991 Train Acc: 0.18866666666666668, Validation Acc: 0.19733333333333333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Note that the accuracy is calculated only on 1500 samples from both train and validation sets, not the entire dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# There are test on entire train/val/test set later\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\src\\models\\gpt1.py:207\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(model, train_data, val_data, learning_rate, batch_size, num_epochs, plot_every, plot)\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_correct \u001b[38;5;241m/\u001b[39m total_count\n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_mae\u001b[39m(model, dataloader: DataLoader) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m    Calculate the mean absolute error for a model over a given dataloader.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m        float: The mean absolute error of the model.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     total_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\Programming\\CSC413\\Salary-Predictor\\src\\models\\gpt1.py:237\u001b[0m, in \u001b[0;36mcalculate_accuracy\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gpt1 import GPT1, train_classifier\n",
    "\n",
    "labels = ['10K-'] + [f\"{i}K - {i + 10}K\" for i in range(10, 150, 10)] + ['160K+']\n",
    "\n",
    "model = GPT1(len(vocab_type) + len(vocab_industry) + len(vocab_state) + len(vocab_level), output_size=len(labels), hidden_size=1000)\n",
    "model.to(device)\n",
    "\n",
    "# Note that the accuracy is calculated only on 1500 samples from both train and validation sets, not the entire dataset\n",
    "# There are test on entire train/val/test set later\n",
    "train_classifier(model, train_data=train_dataset, val_data=val_dataset, learning_rate=0.01, batch_size=20, num_epochs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "583edebc5ddb9524",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copied code to calculate accuracy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def _collate_batch(batch):\n",
    "    \"\"\"Custom collate function for handling batches of data where all input tensors are of the same length.\"\"\"\n",
    "\n",
    "    # Separate and stack the data directly since all tensors are already of the same length\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    categorical_features = torch.stack([item['categorical_features'] for item in batch]).float()\n",
    "    labels = torch.tensor([item['labels'] for item in batch], dtype=torch.float).to(device)\n",
    "\n",
    "    return input_ids, attention_mask, categorical_features, labels\n",
    "\n",
    "def calculate_accuracy(model, dataloader: DataLoader) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the accuracy for a model over a given dataloader.\n",
    "\n",
    "    Args:\n",
    "        model: The model to evaluate.\n",
    "        dataloader (DataLoader): The DataLoader containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model.\n",
    "    \"\"\"\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, categorical_features, labels in dataloader:\n",
    "            outputs = model(input_ids, attention_mask, categorical_features)\n",
    "            outputs = outputs.squeeze()  # Adjust shape if necessary\n",
    "\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            total_correct += torch.sum(predictions == labels).item()\n",
    "            total_count += labels.size(0)\n",
    "        return total_correct / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c269325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model train accuracy: 0.9809\n",
      "Model val accuracy: 0.677\n",
      "Model test accuracy: 0.6742671009771987\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of Base Bert Model without title_emb\n",
    "train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True, collate_fn=_collate_batch)\n",
    "print(f\"Model train accuracy: {calculate_accuracy(model, train_loader)}\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=25, shuffle=True, collate_fn=_collate_batch)\n",
    "print(f\"Model val accuracy: {calculate_accuracy(model, val_loader)}\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=25, shuffle=True, collate_fn=_collate_batch)\n",
    "print(f\"Model test accuracy: {calculate_accuracy(model, test_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
